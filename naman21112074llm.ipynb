{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256},{"sourceId":7018354,"sourceType":"datasetVersion","datasetId":4035516}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\nimport os\nimport gc\n\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n    SentencePieceBPETokenizer\n)\n\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nfrom transformers import PreTrainedTokenizerFast\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    pass\nelse:\n    sub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\n    sub.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\norg_test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\norg_train = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/train_essays.csv')\ndaigt_train = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')\naug_train = pd.read_csv('/kaggle/input/augmented-data-for-llm-detect-ai-generated-text/final_train.csv')\naug_test = pd.read_csv('/kaggle/input/augmented-data-for-llm-detect-ai-generated-text/final_test.csv')\ntrain = daigt_train\ntest = org_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.drop_duplicates(subset=['text'])\ntrain.reset_index(drop=True, inplace=True)\ny_train = train['label'].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LOWERCASE = False\nVOCAB_SIZE = 30522","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Byte-Pair Encoding tokenizer\nraw_tokenizer = SentencePieceBPETokenizer()\n\n# Adding normalization and pre_tokenizer\nraw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\nraw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n# Adding special tokens and creating trainer instance\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n\n# Creating huggingface dataset object\ndataset = Dataset.from_pandas(test[['text']])\n\ndef train_corp_iter():\n    \"\"\"\n    A generator function for iterating over a dataset in chunks.\n    \"\"\"    \n    for i in range(0, len(dataset), 300):\n        yield dataset[i : i + 300][\"text\"]\n\n# Training from iterator REMEMBER it's training on test set...\nraw_tokenizer.train_from_iterator(train_corp_iter())\n\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_object = raw_tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\",\n)\n\ntokenized_texts_test = []\n\n# Tokenize test set with new tokenizer\nfor text in tqdm(test['text'].tolist()):\n    tokenized_texts_test.append(tokenizer.tokenize(text))\n\n\n# Tokenize train set\ntokenized_texts_train = []\n\nfor text in tqdm(train['text'].tolist()):\n    tokenized_texts_train.append(tokenizer.tokenize(text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dummy(text):\n    \"\"\"\n    A dummy function to use as tokenizer for TfidfVectorizer. It returns the text as it is since we already tokenized it.\n    \"\"\"\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting TfidfVectoizer on train set\ndef fitting_vectorizer_on_train(a, b):\n    vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, analyzer = 'word',\n        tokenizer = dummy,\n        preprocessor = dummy,\n        token_pattern = None#, strip_accents='unicode'\n                                )\n\n    vectorizer.fit(a)\n\n    # Getting vocab\n    vocab = vectorizer.vocabulary_\n\n    # Here we fit our vectorizer on train set but this time we use vocabulary from test fit.\n    vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, vocabulary=vocab,\n                                analyzer = 'word',\n                                tokenizer = dummy,\n                                preprocessor = dummy,\n                                token_pattern = None#, strip_accents='unicode'\n                                )\n\n    tf_test = vectorizer.fit_transform(b)\n    tf_train = vectorizer.transform(a)\n\n    del vectorizer\n    gc.collect()\n    return(tf_train, tf_test)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting TfidfVectoizer on test set\ndef fitting_vectorizer_on_test(a, b):\n    vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, analyzer = 'word',\n        tokenizer = dummy,\n        preprocessor = dummy,\n        token_pattern = None#, strip_accents='unicode'\n                                )\n\n    vectorizer.fit(b)\n\n    # Getting vocab\n    vocab = vectorizer.vocabulary_\n\n    # Here we fit our vectorizer on train set but this time we use vocabulary from test fit.\n    vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, vocabulary=vocab,\n                                analyzer = 'word',\n                                tokenizer = dummy,\n                                preprocessor = dummy,\n                                token_pattern = None#, strip_accents='unicode'\n                                )\n\n    tf_train = vectorizer.fit_transform(a)\n    tf_test = vectorizer.transform(b)\n\n    del vectorizer\n    gc.collect()\n    return(tf_train, tf_test)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_voting(tf_train, tf_test, y_train):\n    clf = MultinomialNB(alpha=0.02)\n    lr = LogisticRegression()\n    clf2 = MultinomialNB(alpha=0.01)\n    \n    sgd_model = SGDClassifier(max_iter=6000, tol=1e-4, loss=\"modified_huber\") \n    p6={'n_iter': 2000,'verbose': -1,'objective': 'cross_entropy','metric': 'auc','learning_rate': 0.05073909898961407, \\\n        'colsample_bytree': 0.726023996436955, 'colsample_bynode': 0.5803681307354022, 'lambda_l1': 8.562963348932286, \\\n        'lambda_l2': 4.893256185259296, 'min_data_in_leaf': 115, 'max_depth': 23, 'max_bin': 898}\n    lgb=LGBMClassifier(**p6)\n\n    cat=CatBoostClassifier(\n        iterations=2000,\n        verbose=0,\n        l2_leaf_reg=6.6591278779517808,\n        learning_rate=0.005689066836106983,\n        allow_const_label=True,\n        subsample=0.4,\n        loss_function='CrossEntropy'\n    )\n    \n    weights = [50, 50, 50]\n\n    # Creating the ensemble model\n    ensemble = VotingClassifier(estimators=[\n        ('mnb', clf),\n        ('sgd', sgd_model), \n        ('cat', cat)],\n        weights = [w/sum(weights) for w in weights],\n        voting='soft',\n        n_jobs=-1)\n\n    # Fit the ensemble model\n    ensemble.fit(tf_train, y_train)\n    final_preds = ensemble.predict_proba(tf_test)[:,1]\n    # Garbage collection\n    gc.collect()\n    return(final_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('fitting!')\n# tf_train, tf_test = fitting_vectorizer_on_train(tokenized_texts_train, tokenized_texts_test)  \ntf_train, tf_test = fitting_vectorizer_on_test(tokenized_texts_train, tokenized_texts_test)  \nprint('voting!')\nfinal_preds_submission = calculate_voting(tf_train, tf_test, y_train)\n_ = gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_preds = final_preds_submission\nsub['generated'] = final_preds\nsub.to_csv('submission.csv', index=False)\nsub","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}